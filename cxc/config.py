"""
================================================================================
项目配置文件
================================================================================

本文件包含项目中使用的所有配置参数，用于统一管理项目中的路径设置、
模型配置、数据集配置和运行时参数等。通过环境变量和配置文件来管理
这些参数，便于在不同环境中部署和调试。

文件结构：
- 路径配置：定义项目目录结构和数据文件位置
- 模型配置：指定预训练模型的标识符
- 数据集配置：设置评估使用的数据集信息
- 实验参数：控制评估过程中的各种参数
- 提示词模板：不同类型问题的标准化提示词
- CLIP重排序配置：控制答案重排序功能的开关和阈值
- API配置：管理外部API的访问密钥

使用说明：
1. 直接修改本文件中的配置项以调整项目行为
2. 敏感信息（如API密钥）应通过.env文件设置，避免硬编码
3. 路径配置使用pathlib.Path以确保跨平台兼容性

相关文件：
- .env：存储API密钥等敏感环境变量
- src/vqa_common.py：使用本配置文件的公共功能模块
================================================================================
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# ==============================================================================
# 环境变量加载
# ==============================================================================
# 加载.env文件中的环境变量配置
# override=True参数确保.env文件中的变量会覆盖系统中已存在的同名变量
# 这样可以确保项目配置优先于系统环境变量
load_dotenv(override=True)

# ==============================================================================
# 第一部分：路径配置
# ==============================================================================
# 定义项目目录结构和数据文件存放位置
# 使用pathlib.Path确保跨平台路径兼容性（Windows/Linux/Mac）

# 项目根目录路径 - 指向包含此配置文件的目录
# 通过Path(__file__).parent获取当前文件所在目录的父目录
# __file__是Python内置变量，表示当前脚本的文件路径
ROOT = Path(__file__).parent

# 数据目录配置 - 将数据文件统一存放在data目录下
DATA = ROOT / "data"                    # 主数据目录，包含所有数据相关文件
DATA_IMAGES = DATA / "images"           # 图像数据目录，存放待评估的测试图像
DATA_RESULTS = DATA / "results"         # 评估结果保存目录，存放生成的评估报告和可视化图表

# ==============================================================================
# 第二部分：模型配置
# ==============================================================================
# 配置项目中使用的预训练模型标识符
# 这些标识符用于从Hugging Face Hub或兼容的模型仓库下载模型

MODELS = {
    # CLIP模型配置
    # openai/clip-vit-base-patch32是OpenAI开源的CLIP模型
    # 采用ViT-B/32架构，在大规模图文数据集上训练
    # 用途：图像-文本相似度计算、零样本分类、图文检索
    "clip": "openai/clip-vit-base-patch32",
    
    # Qwen3-VL模型配置
    # Qwen/Qwen3-VL-4B-Instruct是阿里云通义千问开源的视觉语言模型
    # 4B参数版本，适合在消费级GPU上运行
    # 用途：视觉问答、图像理解、多模态推理
    "qwen": "Qwen/Qwen3-VL-4B-Instruct"
}

# ==============================================================================
# 第三部分：数据集配置
# ==============================================================================
# 配置用于评估的数据集信息
# 这里定义了常用的VQA数据集标识符，可根据需要选择使用

DATASET = {
    # TextVQA验证集
    # 来源：Multimodal-Fatima/TextVQA_validation
    # 特点：主要包含需要读取图像中文字的问题，如路牌、标签、显示屏等
    # 用途：测试模型的文字识别和场景理解能力
    "TextVQA": "Multimodal-Fatima/TextVQA_validation",
    
    # VQAv2测试集样本
    # 来源：Multimodal-Fatima/VQAv2_sample_test
    # 特点：包含各种类型的视觉问答问题，是VQA领域最常用的基准数据集
    # 用途：综合评估VQA能力
    "VQAv2": "Multimodal-Fatima/VQAv2_sample_test"
}

# ==============================================================================
# 第四部分：实验参数配置
# ==============================================================================
# 控制评估过程中的各种参数

# 评估样本数量
# 从完整数据集中随机选择指定数量的样本进行评估
# 作用：控制评估速度，完整评估可能耗时较长
# 注意：设置为None表示使用全部样本
SAMPLE_SIZE = 100

# 计算设备自动选择
# 自动检测系统是否有CUDA设备（NVIDIA GPU）
# 如果有GPU则使用cuda，否则使用cpu
# 优先级：通过环境变量CUDA_VISIBLE_DEVICES指定具体GPU
DEVICE = "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu"

# ==============================================================================
# 第五部分：提示词模板配置
# ==============================================================================
# 定义不同类型问题的标准化提示词模板
# 这些模板用于指导模型回答特定类型的问题
# 占位符说明：{}表示需要动态填充的内容

PROMPTS = {
    # 通用VQA问题模板
    # {question}：待回答的问题文本
    "vqa": "请回答关于这张图片的以下问题: {question}",
    
    # 图像描述任务模板
    # 要求模型对图像内容进行详细描述
    "description": "请详细描述这张图片",
    
    # 计数类问题模板
    # {object}：需要计数的目标对象
    "count": "这张图片中有多少个 {object}？",
    
    # 空间关系问题模板
    # {obj1}、{obj2}：需要比较空间关系的两个对象
    "spatial": "这张图片中 {obj1} 和 {obj2} 之间的空间关系是什么？",
    
    # 文字识别问题模板
    # 引导模型识别和读取图像中的文字内容
    "reading": "这张图片中能看到什么文字？"
}

# ==============================================================================
# 第六部分：CLIP重排序配置
# ==============================================================================
# 控制CLIP答案重排序功能的行为
# 重排序：在Qwen3-VL生成答案后，使用CLIP验证和优化答案质量

# 是否启用CLIP重排序功能
# True：启用 - 在VQA推理后使用CLIP进行答案验证和优化
# False：禁用 - 仅使用Qwen3-VL进行VQA推理
# 注意：启用时会增加推理时间，但通常能提高答案质量
CLIP_RERANK = True

# CLIP重排序阈值
# 范围：0.0到1.0之间的浮点数
# 当Qwen3-VL生成的答案与图像的CLIP相似度低于此阈值时
# 系统会考虑使用其他候选答案进行替换
# 设置建议：
# - 较低值（0.2-0.3）：更积极地替换答案，可能提高准确率但增加误判
# - 较高值（0.4-0.5）：保守策略，减少错误替换但可能错过优化机会
CLIP_THRESHOLD = 0.3

# ==============================================================================
# 第七部分：API配置
# ==============================================================================
# 配置外部API的访问凭证

# DashScope API密钥
# 用于调用阿里云通义千问的Qwen3-VL API服务
# 获取方式：登录阿里云控制台 -> 搜索DashScope -> 创建API密钥
# 安全建议：不要直接硬编码在此文件中，应使用.env文件设置
# 相关环境变量：DASHSCOPE_API_KEY
API_KEY = os.getenv("DASHSCOPE_API_KEY")

# 检查API密钥是否配置
if API_KEY is None:
    print("=" * 60)
    print("警告：未检测到DASHSCOPE_API_KEY环境变量！")
    print("请在.env文件中设置DASHSCOPE_API_KEY以使用Qwen3-VL API")
    print("=" * 60)